{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize the MediaPipe Pose and Drawing utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize BlazePose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB for MediaPipe Pose\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb.flags.writeable = False\n",
    "\n",
    "    # Run BlazePose detection\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    # Convert back to BGR for OpenCV\n",
    "    image_rgb.flags.writeable = True\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw the pose landmarks on the frame\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_bgr, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),  # For landmarks\n",
    "            mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)  # For connections\n",
    "        )\n",
    "\n",
    "    # Show the frame with BlazePose landmarks\n",
    "    cv2.imshow('BlazePose on Webcam', image_bgr)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Variables to track push-up count and state\n",
    "pushup_count = 0\n",
    "is_down_position = False  # Indicates if the user is in the \"down\" part of the push-up\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"Calculates the angle between three points (in degrees).\"\"\"\n",
    "    a = np.array(a)  # First point\n",
    "    b = np.array(b)  # Mid point\n",
    "    c = np.array(c)  # End point\n",
    "\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "\n",
    "    if angle > 180.0:\n",
    "        angle = 360.0 - angle\n",
    "\n",
    "    return angle\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb.flags.writeable = False\n",
    "\n",
    "    # Run BlazePose\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    # Convert back to BGR for OpenCV\n",
    "    image_rgb.flags.writeable = True\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # If landmarks are detected\n",
    "    if results.pose_landmarks:\n",
    "        # Draw pose landmarks on the image\n",
    "        mp_drawing.draw_landmarks(image_bgr, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Get landmark positions\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "        # Extract keypoints for shoulder, elbow, and wrist\n",
    "        left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                         landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "        left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                      landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "        left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                      landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "\n",
    "        # Calculate the angle at the left elbow\n",
    "        elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "\n",
    "        # Detect the push-up movement\n",
    "        if elbow_angle < 90:  # The \"down\" position (arms bent)\n",
    "            is_down_position = True\n",
    "        elif elbow_angle > 160 and is_down_position:  # The \"up\" position (arms straight)\n",
    "            pushup_count += 1\n",
    "            is_down_position = False\n",
    "\n",
    "        # Display the count on the image\n",
    "        cv2.putText(image_bgr, f'Push-ups: {pushup_count}', (50, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Push-up Counter', image_bgr)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dougl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Initialize MediaPipe BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Store keypoints over time\n",
    "keypoints_list = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb.flags.writeable = False\n",
    "\n",
    "    # Run BlazePose\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    # Convert back to BGR for OpenCV\n",
    "    image_rgb.flags.writeable = True\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # If landmarks are detected\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        # Extract and store the keypoints (x, y, z) and visibility for each joint\n",
    "        keypoints = []\n",
    "        for landmark in landmarks:\n",
    "            keypoints.append((landmark.x, landmark.y, landmark.z, landmark.visibility))\n",
    "        \n",
    "        keypoints_list.append(keypoints)\n",
    "\n",
    "        # Draw the pose landmarks on the frame (for visualization)\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image_bgr, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS\n",
    "        )\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Skeleton Capture', image_bgr)\n",
    "\n",
    "    # Press q to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "# Save keypoints_list to a JSON file\n",
    "with open('skeleton_data.json', 'w') as f:\n",
    "    json.dump(keypoints_list, f)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define keypoint connections (from BlazePose)\n",
    "keypoint_connections = [\n",
    "    (6,5),\n",
    "    (4,5),\n",
    "    (6,8),\n",
    "\n",
    "    (1,2),\n",
    "    (2,3),\n",
    "    (3,7),\n",
    "\n",
    "    (10,9),\n",
    "\n",
    "    (20,18),\n",
    "    (18,16),\n",
    "    (16,22),\n",
    "    (16,20),\n",
    "\n",
    "    (16,14),\n",
    "    (14,12),\n",
    "    (12,11),\n",
    "    (11,13),\n",
    "    (13,15),\n",
    "    (15,21),\n",
    "    (15,19),\n",
    "    (15,17),\n",
    "    (19,17),\n",
    "\n",
    "    (12,24),\n",
    "    (11,23),\n",
    "    (24,23),\n",
    "\n",
    "    (24,26),\n",
    "    (26,28),\n",
    "    (32,28),\n",
    "    (30,28),\n",
    "    (32,30),\n",
    "    \n",
    "    (23,25),\n",
    "    (27,25),\n",
    "    (27,29),\n",
    "    (29,31),\n",
    "    (27,31)\n",
    "]\n",
    "\n",
    "# Load keypoints from JSON\n",
    "with open('skeleton_data.json', 'r') as f:\n",
    "    loaded_keypoints_list = json.load(f)\n",
    "\n",
    "# Create a window to replay the movements\n",
    "window_name = 'Skeleton Replay'\n",
    "cv2.namedWindow(window_name)\n",
    "\n",
    "# Replay the keypoints data\n",
    "for keypoints in loaded_keypoints_list:\n",
    "    # Create a blank frame\n",
    "    frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Scale the keypoints to fit the window size (assuming original BlazePose keypoints are normalized between 0 and 1)\n",
    "    height, width = frame.shape[:2]\n",
    "    scaled_keypoints = []\n",
    "    for i, (x, y, z, visibility) in enumerate(keypoints):\n",
    "        if visibility > 0.4:  # Only show keypoints with good visibility\n",
    "            cx, cy = int(x * width), int(y * height)\n",
    "            scaled_keypoints.append((cx, cy))  # Store for drawing lines\n",
    "            # Draw keypoint as a small circle\n",
    "            cv2.circle(frame, (cx, cy), 5, (255, 255, 255), -1)\n",
    "        else:\n",
    "            scaled_keypoints.append(None)  # If not visible, mark as None\n",
    "    \n",
    "    # Draw the skeleton by connecting keypoints\n",
    "    for start_idx, end_idx in keypoint_connections:\n",
    "        if scaled_keypoints[start_idx] is not None and scaled_keypoints[end_idx] is not None:\n",
    "            cv2.line(frame, scaled_keypoints[start_idx], scaled_keypoints[end_idx], (0, 255, 0), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(window_name, frame)\n",
    "\n",
    "    if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "\n",
    "# Initialize MediaPipe BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Load video file\n",
    "video_path = 'workout.mp4'  # Change to your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Store keypoints over time\n",
    "keypoints_list = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Video has ended.\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb.flags.writeable = False\n",
    "\n",
    "    # Run BlazePose\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    # If landmarks are detected\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "        # Extract and store the keypoints (x, y, z) and visibility for each joint\n",
    "        keypoints = []\n",
    "        for landmark in landmarks:\n",
    "            keypoints.append((landmark.x, landmark.y, landmark.z, landmark.visibility))\n",
    "        \n",
    "        keypoints_list.append(keypoints)\n",
    "\n",
    "    # Press 'q' to exit early\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Save keypoints_list to a JSON file\n",
    "with open('video_skeleton_data.json', 'w') as f:\n",
    "    json.dump(keypoints_list, f)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define keypoint connections (from BlazePose)\n",
    "keypoint_connections = [\n",
    "    (6,5),\n",
    "    (4,5),\n",
    "    (6,8),\n",
    "\n",
    "    (1,2),\n",
    "    (2,3),\n",
    "    (3,7),\n",
    "\n",
    "    (10,9),\n",
    "\n",
    "    (20,18),\n",
    "    (18,16),\n",
    "    (16,22),\n",
    "    (16,20),\n",
    "\n",
    "    (16,14),\n",
    "    (14,12),\n",
    "    (12,11),\n",
    "    (11,13),\n",
    "    (13,15),\n",
    "    (15,21),\n",
    "    (15,19),\n",
    "    (15,17),\n",
    "    (19,17),\n",
    "\n",
    "    (12,24),\n",
    "    (11,23),\n",
    "    (24,23),\n",
    "\n",
    "    (24,26),\n",
    "    (26,28),\n",
    "    (32,28),\n",
    "    (30,28),\n",
    "    (32,30),\n",
    "    \n",
    "    (23,25),\n",
    "    (27,25),\n",
    "    (27,29),\n",
    "    (29,31),\n",
    "    (27,31)\n",
    "]\n",
    "\n",
    "# Load keypoints from the previously saved JSON file\n",
    "with open('video_skeleton_data.json', 'r') as f:\n",
    "    loaded_keypoints_list = json.load(f)\n",
    "\n",
    "# Load video again for playback\n",
    "video_path = 'your_video.mp4'  # Change to your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Replay skeleton over video frames\n",
    "for keypoints in loaded_keypoints_list:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Video has ended.\")\n",
    "        break\n",
    "\n",
    "    height, width = frame.shape[:2]\n",
    "    \n",
    "    # Scale the keypoints to fit the video frame size\n",
    "    scaled_keypoints = []\n",
    "    for i, (x, y, z, visibility) in enumerate(keypoints):\n",
    "        if visibility > 0.5:  # Only show keypoints with good visibility\n",
    "            cx, cy = int(x * width), int(y * height)\n",
    "            scaled_keypoints.append((cx, cy))  # Store for drawing lines\n",
    "            # Draw keypoint as a small circle\n",
    "            cv2.circle(frame, (cx, cy), 5, (255, 255, 255), -1)\n",
    "        else:\n",
    "            scaled_keypoints.append(None)  # If not visible, mark as None\n",
    "    \n",
    "    # Draw the skeleton by connecting keypoints\n",
    "    for start_idx, end_idx in keypoint_connections:\n",
    "        if scaled_keypoints[start_idx] is not None and scaled_keypoints[end_idx] is not None:\n",
    "            cv2.line(frame, scaled_keypoints[start_idx], scaled_keypoints[end_idx], (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the frame with the skeleton\n",
    "    cv2.imshow('Skeleton Replay', frame)\n",
    "\n",
    "    # Press 'q' to exit early\n",
    "    if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from mediapipe import solutions\n",
    "\n",
    "# Initialize BlazePose\n",
    "pose = solutions.pose.Pose()\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)  # Shoulder\n",
    "    b = np.array(b)  # Elbow\n",
    "    c = np.array(c)  # Wrist\n",
    "    \n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "    \n",
    "    return angle\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "stage = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)\n",
    "    \n",
    "    # Get the frame's height and width for pixel conversion\n",
    "    height, width, _ = frame.shape\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        # Get coordinates for key landmarks\n",
    "        shoulder_left = [landmarks[11].x * width, landmarks[11].y * height]\n",
    "        elbow_left = [landmarks[13].x * width, landmarks[13].y * height]\n",
    "        wrist_left = [landmarks[15].x * width, landmarks[15].y * height]\n",
    "        \n",
    "        # Calculate the elbow angle\n",
    "        elbow_angle = calculate_angle(shoulder_left, elbow_left, wrist_left)\n",
    "        \n",
    "        # Push-up counter logic\n",
    "        if elbow_angle > 160:\n",
    "            stage = \"up\"\n",
    "        if elbow_angle < 90 and stage == \"up\":\n",
    "            stage = \"down\"\n",
    "            count += 1\n",
    "            print(f\"Push-up count: {count}\")\n",
    "        \n",
    "        # Find the min and max coordinates to create a bounding box\n",
    "        x_values = [landmark.x for landmark in landmarks]\n",
    "        y_values = [landmark.y for landmark in landmarks]\n",
    "        \n",
    "        # Convert normalized coordinates to pixel values\n",
    "        x_min = int(min(x_values) * width)\n",
    "        x_max = int(max(x_values) * width)\n",
    "        y_min = int(min(y_values) * height)\n",
    "        y_max = int(max(y_values) * height)\n",
    "        \n",
    "        # Draw the bounding box around the person\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the frame with the bounding box and push-up counter\n",
    "    cv2.putText(frame, f'Push-Up Count: {count}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Push-Up Counter', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
